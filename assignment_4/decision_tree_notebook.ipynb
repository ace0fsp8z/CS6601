{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# CS 6601 Assignment 4: Decision Tree Learning\n",
    "\n",
    "Machine learning offers a number of methods for classifying data into discrete categories, such as k-means clustering. Decision trees provide a structure for such categorization, based on a series of decisions that led to separate distinct outcomes. In this assignment, you will work with decision trees to perform binary classification according to some decision boundary.  Your challenge is to build and to train decision trees capable of solving useful classification problems. You will learn first how to build decision trees, then how to effectively train them and finally how to test their performance.\n",
    "\n",
    "This assignment is due on T-Square as decision_trees.py on March 19th midnight AOE (anywhere on Earth).\n",
    "\n",
    "<img src=\"files/dt.png\" width=\"50%\" align=\"middle\">\n",
    "\n",
    "Abstract\n",
    "-------\n",
    "You will build, train and test decision tree models to perform basic classification tasks.\n",
    "\n",
    "Motivation\n",
    "-------\n",
    "Classification is used widely in machine learning to figure out how to sort new data that comes through.\n",
    "\n",
    "Objectives\n",
    "-------\n",
    "Students should understand how decision trees and random forests work.\n",
    "Students should develop and intuition for how and why accuracy differs for training and testing data based on different parameters.\n",
    "\n",
    "Evaluation\n",
    "-------\n",
    "Evaluation is using the last submission on Bonnie.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Datasets\n",
    "-------\n",
    "\n",
    "We have provided you with two datasets:\n",
    "    \n",
    "    part23_data.csv: 4 features, 1372 data points, binary classification (last column) \n",
    "        \n",
    "    challenge_train.csv: 30 features, 6636 datapoints, binary classification (first column)\n",
    "        \n",
    "    challenge_test.csv: not provided, similar to challenge_train but with 40% of the datapoints\n",
    "\n",
    "Imports\n",
    "-------\n",
    "\n",
    "We are only allowing three imports: numpy, collections.Counter and time. We will be checking to see if any other libraries are used. You are not allowed to use any outside libraries especially for part 4 (challenge)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Introduction\n",
    "-------\n",
    "\n",
    "For this assignment we're going to need an explicit way to make structured decisions. The following is `DecisionNode`,  representing a decision node as some atomic choice in a binary decision graph. It can represent a class label (i.e. a final decision) or a binary decision to guide the us through a flow-chart to arrive at a decision. Note that in this representation 'True' values for a decision take us to the left. This is arbitrary but matters for what comes next.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "class DecisionNode():\n",
    "    \"\"\"Class to represent a single node in\n",
    "    a decision tree.\"\"\"\n",
    "\n",
    "    def __init__(self, left, right, decision_function, class_label=None):\n",
    "        \"\"\"Create a node with a left child, right child,\n",
    "        decision function and optional class label\n",
    "        for leaf nodes.\"\"\"\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.decision_function = decision_function\n",
    "        self.class_label = class_label\n",
    "\n",
    "    def decide(self, feature):\n",
    "        \"\"\"Return on a label if node is leaf,\n",
    "        or pass the decision down to the node's\n",
    "        left/right child (depending on decision\n",
    "        function).\"\"\"\n",
    "        if self.class_label is not None:\n",
    "            return self.class_label\n",
    "        elif self.decision_function(feature):\n",
    "            return self.left.decide(feature)\n",
    "        else:\n",
    "            return self.right.decide(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Part 1a: Building a Binary Tree by Hand\n",
    "--------\n",
    "5 pts.\n",
    "\n",
    "In `build_decision_tree()`, construct a tree of decision nodes by hand in order to classify the data below, i.e. map each datum $x$ to a label $y$. Select tests to be as small as possible (in terms of attributes), breaking ties among tests with the same number of attributes by selecting the one that classifies the greatest number of examples correctly. If multiple tests have the same number of attributes and classify the same number of examples, then break the tie using attributes with lower index numbers (e.g. select $A_1$ over $A_2$)\n",
    "\n",
    "| Datum  | $A_1$ | $A_2$ | $A_3$ | $A_4$ |  y  |\n",
    "| -------| :---: | :---: | :---: | :---: | ---:|\n",
    "| $x_1$  |   1   |   0   |   0   |   0   |  1  |\n",
    "| $x_2$  |   1   |   0   |   1   |   1   |  1  |\n",
    "| $x_3$  |   0   |   1   |   0   |   0   |  1  |\n",
    "| $x_4$  |   0   |   1   |   1   |   0   |  0  |\n",
    "| $x_5$  |   1   |   1   |   0   |   1   |  1  |\n",
    "| $x_6$  |   0   |   1   |   0   |   1   |  0  |\n",
    "| $x_7$  |   0   |   0   |   1   |   1   |  1  |\n",
    "| $x_8$  |   0   |   0   |   1   |   0   |  0  |\n",
    "\n",
    "Hints: \n",
    "To get started, it might help to <b>draw out the tree by hand</b> with each attribute representing a node.\n",
    "\n",
    "To create the decision function to pass to your `DecisionNode`, you can create a lambda expression as follows:\n",
    "\n",
    "    func = lambda feature : feature[2] == 0\n",
    "    \n",
    "in which we would choose the left node if the third attribute is 0.\n",
    "\n",
    "For example, if your tree looked like this: if A1=0 then class = 1; else class = 0\n",
    "\n",
    "       A1\n",
    "     /   \\\n",
    "    1      0\n",
    "   \n",
    "You would write your code like this:\n",
    "\n",
    "    decision_tree_root= DecisionNode(None, None, lambda a1: a1 == 0)\n",
    "    decision_tree_root.left = DecisionNode(None, None, None, 1)\n",
    "    decision_tree_root.right = DecisionNode(None, None, None, 0)\n",
    "    return decision_tree_root\n",
    "    \n",
    "\n",
    "Requirements: The tree nodes should be less than 10 nodes including the leaf (not the number of instances, but the actual nodes in the tree).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "examples = [[1,0,0,0],\n",
    "            [1,0,1,1],\n",
    "            [0,1,0,0],\n",
    "            [0,1,1,0],\n",
    "            [1,1,0,1],\n",
    "            [0,1,0,1],\n",
    "            [0,0,1,1],\n",
    "            [0,0,1,0]]\n",
    "\n",
    "classes = [1,1,1,0,1,0,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_decision_tree():\n",
    "    \"\"\"Create decision tree\n",
    "    capable of handling the provided \n",
    "    data.\"\"\"\n",
    "    # TODO: build full tree from root\n",
    "    decision_tree_root = DecisionNode(None, None, lambda feature: feature[0] == 1)\n",
    "    decision_tree_root.left = DecisionNode(None, None, None, 1)\n",
    "    decision_tree_root.right = DecisionNode(None, None, lambda feature: feature[2] == 1)\n",
    "\n",
    "    a3 = decision_tree_root.right\n",
    "    a3.left = DecisionNode(None, None, lambda feature: feature[3] == 1)\n",
    "    a3.right = DecisionNode(None, None, lambda feature: feature[3] == 0)\n",
    "\n",
    "    a4_1 = a3.left\n",
    "    a4_1.left = DecisionNode(None, None, None, 1)\n",
    "    a4_1.right = DecisionNode(None, None, None, 0)\n",
    "\n",
    "    a4_2 = a3.right\n",
    "    a4_2.left = DecisionNode(None, None, None, 1)\n",
    "    a4_2.right = DecisionNode(None, None, None, 0)\n",
    "\n",
    "    return decision_tree_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "decision_tree_root = build_decision_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Part 1b: Precision, Recall, Accuracy and Confusion Matrix\n",
    "--------\n",
    "12 pts.\n",
    "\n",
    "Now that we have a decision tree, we're going to need some way to evaluate its performance. In most cases we'd reserve a portion of the training data for evaluation, or use cross-validation. For now let's just see how your tree does on the provided examples. In the stubbed out code below, fill out the methods to compute the confusion matrix, accuracy, precision and recall for your classifier output. `classifier_output` is just the list of labels that your classifier outputs, corresponding to the same examples as `true_labels`. You can refer to Wikipedia for calculating the true/false positive/negative.\n",
    "\n",
    "You should get 1.0 (float) for precision, recall and accuracy.\n",
    "\n",
    "You can create a simple example for the confusion matrix for testing purposes and count by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(classifier_output, true_labels):\n",
    "    #TODO output should be [[true_positive, false_negative], [false_positive, true_negative]]\n",
    "    #TODO output is a list\n",
    "    output = [\n",
    "        [0, 0],\n",
    "        [0, 0]\n",
    "    ]\n",
    "    for i, actual in enumerate(true_labels):\n",
    "        prediction = classifier_output[i]\n",
    "        if actual == 1:\n",
    "            if prediction == actual:\n",
    "                output[0][0] += 1\n",
    "            else:\n",
    "                output[0][1] += 1\n",
    "        elif actual == 0:\n",
    "            if prediction == actual:\n",
    "                output[1][1] += 1\n",
    "            else:\n",
    "                output[1][0] += 1\n",
    "    return output\n",
    "\n",
    "\n",
    "def precision(classifier_output, true_labels):\n",
    "    #TODO precision is measured as: true_positive/ (true_positive + false_positive)\n",
    "    [tp, _], [fp, _] = confusion_matrix(classifier_output, true_labels)\n",
    "    return tp / float(tp + fp)\n",
    "    \n",
    "    \n",
    "def recall(classifier_output, true_labels):\n",
    "    #TODO: recall is measured as: true_positive/ (true_positive + false_negative)\n",
    "    [tp, fn], _ = confusion_matrix(classifier_output, true_labels)\n",
    "    return tp / float(tp + fn)\n",
    "    \n",
    "def accuracy(classifier_output, true_labels):\n",
    "    #TODO accuracy is measured as:  correct_classifications / total_number_examples\n",
    "    [tp, fn], [fp, tn] = confusion_matrix(classifier_output, true_labels)\n",
    "    return (tp + tn) / float(tp + fn + fp + tn)\n",
    "    \n",
    "classifier_output = [decision_tree_root.decide(example) for example in examples]\n",
    "  \n",
    "p1_confusion_matrix = confusion_matrix(classifier_output, classes)\n",
    "p1_accuracy = accuracy( classifier_output, classes )\n",
    "p1_precision = precision(classifier_output, classes)\n",
    "p1_recall = recall(classifier_output, classes)\n",
    "\n",
    "print p1_confusion_matrix, p1_accuracy, p1_precision, p1_recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Part 2a: Decision Tree Learning\n",
    "-------\n",
    "6 pts.\n",
    "\n",
    "You will need to implement `entropy()` and `information_gain()` in order to do so (hints [here](https://en.wikipedia.org/wiki/Entropy_(information_theory)) and [here](https://en.wikipedia.org/wiki/Information_gain_in_decision_trees)). Test cases have been provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def entropy(class_vector):\n",
    "    \"\"\"Compute the entropy for a list\n",
    "    of classes (given as either 0 or 1).\"\"\"\n",
    "    # TODO: finish this\n",
    "    # H(x) = -∑p(x_i) log2(p(x_i))\n",
    "    total = len(class_vector) or 1\n",
    "    p = sum(class_vector) / float(total)\n",
    "    x1 = p * np.log2(p) if p > 0 else 0\n",
    "    x2 = (1 - p) * np.log2(1 - p) if p < 1 else 0\n",
    "    H = -1 * (x1 + x2)\n",
    "    return H\n",
    "\n",
    "    \n",
    "def information_gain(previous_classes, current_classes ):\n",
    "    \"\"\"Compute the information gain between the\n",
    "    previous and current classes (a list of \n",
    "    lists where each list has 0 and 1 values).\"\"\"\n",
    "    # TODO: finish this\n",
    "    # Remainder(A) = ∑((pk + nk) / (p + n)) * B(pk / (pk + nk))\n",
    "    # Gain(A) = B(p / (p + n) - Remainder(A))\n",
    "    total = len(previous_classes)\n",
    "    \n",
    "    remainder = 0\n",
    "    for current_class in current_classes:\n",
    "        positive = sum(current_class)\n",
    "        remainder += (len(current_class) / float(total)) * entropy(current_class)\n",
    "    \n",
    "    gain = entropy(previous_classes) - remainder\n",
    "    return gain\n",
    "\n",
    "\n",
    "def test_information_gain():\n",
    "   \"\"\" Assumes information_gain() accepts (classes, [list of subclasses])\n",
    "       Feel free to edit / enhance this note with more tests \"\"\"\n",
    "   restaurants = [0]*6 + [1]*6\n",
    "   split_patrons =   [[0,0], [1,1,1,1], [1,1,0,0,0,0]]\n",
    "   split_food_type = [[0,1],[0,1],[0,0,1,1],[0,0,1,1]]\n",
    "    \n",
    "   # If you're using numpy indexing add the following before calling information_gain()\n",
    "   # split_patrons =   [np.array(i) for i in split_patrons]   #convert to np array \n",
    "   # split_food_type = [np.array(i) for i in split_food_type]\n",
    "\n",
    "   gain_patrons = information_gain(restaurants, split_patrons)\n",
    "   gain_type = information_gain(restaurants, split_food_type)\n",
    "   assert round(gain_patrons,3) == 0.541, \"Information Gain on patrons should be 0.541\"\n",
    "   assert gain_type == 0.0, \"Information gain on type should be 0.0\"\n",
    "   print \"Information Gain calculations correct...\"\n",
    "    \n",
    "   assert (information_gain([1,1,1,0,0,0],[[1,1,1],[0,0,0]])==1),\"TEST FAILED\"\n",
    "   assert (round(information_gain([1,1,1,0,0,0],[[1,1,0],[1,0,0]]),2)==0.08),\"TEST FAILED\"\n",
    "\n",
    "def test_entropy(): \n",
    "    assert (entropy([1,1,1,0,0,0])==1),\"TEST FAILED\"\n",
    "    assert (entropy([1,1,1,1,1,1])==0),\"TEST FAILED\"\n",
    "    assert (int(entropy([1,1,0,0,0,0])*100)==91),\"TEST FAILED\"\n",
    "    \n",
    "test_information_gain()\n",
    "test_entropy() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"files/infogain.png\" width=\"50%\" align=\"middle\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Part 2b: Decision Tree Learning\n",
    "-------\n",
    "20 pts.\n",
    "\n",
    "<ul><li>File to use: part23_data.csv</li>\n",
    "<li>Grading: average test accuracy over 10 rounds should be >= 70%</li></ul>\n",
    "\n",
    "As the size of our training set grows, it rapidly becomes impractical to build these trees by hand. We need a procedure to automagically construct these trees.\n",
    "\n",
    "For starters, let's consider the following algorithm (a variation of [C4.5](https://en.wikipedia.org/wiki/C4.5_algorithm)) for the construction of a decision tree from a given set of examples:\n",
    "\n",
    "    1) Check for base cases: \n",
    "         a) If all elements of a list are of the same class, return a leaf node with the appropriate class label.\n",
    "         b) If a specified depth limit is reached, return a leaf labeled with the most frequent class.\n",
    "\n",
    "    2) For each attribute alpha: evaluate the normalized information gain gained by splitting on attribute $\\alpha$\n",
    "\n",
    "    3) Let alpha_best be the attribute with the highest normalized information gain\n",
    "\n",
    "    4) Create a decision node that splits on alpha_best\n",
    "\n",
    "    5) Recur on the sublists obtained by splitting on alpha_best, and add those nodes as children of node\n",
    "\n",
    "First, in the `DecisionTree.__build_tree__()` method implement the above algorithm. \n",
    "\n",
    "Next, in `DecisionTree.classify()` below, write a function to produce classifications for a list of features once your decision tree has been built.\n",
    "\n",
    "Some other helpful notes:\n",
    "\n",
    "    1) Your features and classify should be in numpy arrays where if the dataset was (m x n) then the features is (m x n-1) and classify is (m x 1).\n",
    "\n",
    "    2) These features are continuous features and you will need to split based on a threshold.\n",
    "    \n",
    "How grading works:\n",
    "\n",
    "    1) We load part23_data.csv and create our cross-validation training and test set with a k=10 folds. \n",
    "    \n",
    "    2) We classify the training data onto the three then fit the testing data onto the tree. \n",
    "    \n",
    "    3) We check the accuracy of your results versus the true results and we return the average of this over 10 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    \"\"\"Class for automatic tree-building\n",
    "    and classification.\"\"\"\n",
    "\n",
    "    def __init__(self, depth_limit=float('inf')):\n",
    "        \"\"\"Create a decision tree with an empty root\n",
    "        and the specified depth limit.\"\"\"\n",
    "        self.root = None\n",
    "        self.depth_limit = depth_limit\n",
    "\n",
    "    def fit(self, features, classes):\n",
    "        \"\"\"Build the tree from root using __build_tree__().\"\"\"\n",
    "        classes = np.array(classes)\n",
    "        self.root = self.__build_tree__(features, classes)\n",
    "\n",
    "    def __build_tree__(self, features, classes, depth=0):  \n",
    "        \"\"\"Implement the above algorithm to build\n",
    "        the decision tree using the given features and\n",
    "        classes to build the decision functions.\"\"\"\n",
    "        #TODO: finish this\n",
    "        # 1) Check for base cases:\n",
    "        #      a) If all elements of a list are of the same class, return a leaf node with the appropriate class label.\n",
    "        #      b) If a specified depth limit is reached, return a leaf labeled with the most frequent class.\n",
    "        # 2) For each attribute alpha: evaluate the normalized information gain gained by splitting on attribute $\\alpha$\n",
    "        # 3) Let alpha_best be the attribute with the highest normalized information gain\n",
    "        # 4) Create a decision node that splits on alpha_best\n",
    "        # 5) Recur on the sublists obtained by splitting on alpha_best, and add those nodes as children of node\n",
    "        if np.all(np.array(classes) == 1):\n",
    "            return DecisionNode(None, None, None, 1)\n",
    "        elif np.all(np.array(classes) == 0):\n",
    "            return DecisionNode(None, None, None, 0)\n",
    "        if depth >= self.depth_limit or depth >= len(features):\n",
    "            if sum(classes) > float(len(classes) / 2):\n",
    "                return DecisionNode(None, None, None, 1)\n",
    "            else:\n",
    "                return DecisionNode(None, None, None, 0)\n",
    "\n",
    "        # if set(np.unique(features)) == set([0, 1]):\n",
    "        #     continue\n",
    "        # else:\n",
    "        #     # continuous value\n",
    "\n",
    "        # find alpha best by using variance -2 stddev to 2 stddev\n",
    "\n",
    "        # xn = []\n",
    "        # num_features = features.shape[1]\n",
    "        # min_stddev = -2\n",
    "        # max_stddev = 3\n",
    "        # variance = max_stddev - min_stddev\n",
    "        # for col in xrange(num_features):\n",
    "        #     xn.append(xrange(min_stddev, max_stddev))\n",
    "        #\n",
    "        # combination = np.array(np.meshgrid(*xn, indexing='ij'))\n",
    "        # combination = np.rollaxis(combination, 0, variance)\n",
    "        # combination = combination.reshape(variance ** num_features, num_features)\n",
    "\n",
    "        alpha_best = None\n",
    "        beta_gain_list = []\n",
    "        beta_best_list = []\n",
    "        best_gain = np.float('-inf')\n",
    "        variance_list = np.std(features, axis=0)\n",
    "        # for each feature, we want to find the best threshold\n",
    "        for i in xrange(features.shape[1]):\n",
    "            alpha = features[:, i]\n",
    "            mean = np.mean(alpha)\n",
    "            variance = variance_list[i]\n",
    "            beta_best = None\n",
    "            beta_gain = np.float('-inf')\n",
    "            for stddev in xrange(-2, 3):\n",
    "                beta = alpha.copy()\n",
    "                threshold = mean + stddev * variance\n",
    "                indices = beta > threshold\n",
    "                not_indices = np.negative(indices)\n",
    "                beta[:] = 0\n",
    "                beta[indices] = 1\n",
    "                gain = information_gain(classes, [classes[indices], classes[not_indices]])\n",
    "                # print stddev, gain\n",
    "                if gain > beta_gain:\n",
    "                    beta_gain = gain\n",
    "                    beta_best = stddev\n",
    "            beta_gain_list.append(beta_gain)\n",
    "            beta_best_list.append(beta_best)\n",
    "\n",
    "        alpha_best = np.argmax(beta_gain_list)\n",
    "        alpha = features[:, alpha_best]\n",
    "        threshold = np.mean(alpha) + beta_best_list[alpha_best] * variance_list[alpha_best]\n",
    "        indices = alpha > threshold\n",
    "        not_indices = np.negative(indices)\n",
    "        # print 'alpha_best', alpha_best\n",
    "        # print 'depth', depth\n",
    "\n",
    "        left_features = features[indices]\n",
    "        right_features = features[not_indices]\n",
    "        left_classes = classes[indices]\n",
    "        right_classes = classes[not_indices]\n",
    "        left_node = self.__build_tree__(left_features, left_classes, depth + 1)\n",
    "        right_node = self.__build_tree__(right_features, right_classes, depth + 1)\n",
    "\n",
    "        return DecisionNode(left_node, right_node, lambda features: features[alpha_best] > threshold)\n",
    "        \n",
    "    def classify(self, features):\n",
    "        \"\"\"Use the fitted tree to \n",
    "        classify a list of examples. \n",
    "        Return a list of class labels.\"\"\"\n",
    "        class_labels = [self.root.decide(feature) for feature in features]\n",
    "        return class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Part 2c: Validation\n",
    "--------\n",
    "10 pts.\n",
    "\n",
    "<ul><li>File to use: part23_data.csv</li>\n",
    "\n",
    "<li>Grading: average test accuracy over 10 rounds should be >= 70%</li></ul>\n",
    "\n",
    "In general, reserving part of your data as a test set can lead to unpredictable performance- a serendipitous choice of your train or test split could give you a very inaccurate idea of how your classifier performs. That's where k-fold cross validation comes in.\n",
    "\n",
    "In `generate_k_folds()`, we'll split the dataset at random into k equal subsections. Then iterating on each of our k samples, we'll reserve that sample for testing and use the other k-1 for training. Averaging the results of each fold should give us a more consistent idea of how the classifier is doing across the data as a whole.\n",
    "\n",
    "How grading works: The same as 2b however, we use your generate_k_folds instead of ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_csv(data_file_path, class_index):\n",
    "    handle = open(data_file_path, 'r')\n",
    "    contents = handle.read()\n",
    "    handle.close()\n",
    "    rows = contents.split('\\n')\n",
    "    out = np.array([[float(i) for i in r.split(',')] for r in rows if r])\n",
    "    \n",
    "    if(class_index == -1):\n",
    "        # this is used for part23_data\n",
    "        classes= map(int,  out[:,class_index])\n",
    "        features = out[:,:class_index]\n",
    "        return features, classes\n",
    "    elif(class_index == 0):\n",
    "        # this is used for challenge_train\n",
    "        classes= map(int,  out[:, class_index])\n",
    "        features = out[:, 1:]\n",
    "        return features, classes\n",
    "    else:\n",
    "        # this is used for vectorize\n",
    "        return out\n",
    "\n",
    "def generate_k_folds(dataset, k):\n",
    "    #TODO this method should return a list of folds,\n",
    "    # where each fold is a tuple like (training_set, test_set)\n",
    "    # where each set is a tuple like (examples, classes)\n",
    "    features, classes = dataset\n",
    "    classes = np.array(classes)\n",
    "\n",
    "    indices = np.arange(classes.size)\n",
    "    training_size = int(.9 * classes.size)\n",
    "\n",
    "    folds = []\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    for i in xrange(k):\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        randomized_features = features[indices]\n",
    "        randomized_classes = classes[indices]\n",
    "\n",
    "        training_features = randomized_features[:training_size]\n",
    "        training_classes = randomized_classes[:training_size]\n",
    "        training_set = (training_features, training_classes)\n",
    "\n",
    "        test_features = randomized_features[training_size:]\n",
    "        test_classes = randomized_classes[training_size:]\n",
    "        test_set = (test_features, test_classes)\n",
    "\n",
    "        folds.append((training_set, test_set))\n",
    "\n",
    "    return folds\n",
    "\n",
    "\n",
    "dataset = load_csv('part23_data.csv', -1)\n",
    "ten_folds = generate_k_folds(dataset, 10)\n",
    "\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "confusion = []\n",
    "\n",
    "for fold in ten_folds:\n",
    "    train, test = fold\n",
    "    train_features, train_classes = train\n",
    "    test_features, test_classes = test\n",
    "    tree = DecisionTree( )\n",
    "    tree.fit( train_features, train_classes)\n",
    "    output = tree.classify(test_features)\n",
    "    \n",
    "    accuracies.append( accuracy(output, test_classes))\n",
    "    precisions.append( precision(output, test_classes))\n",
    "    recalls.append( recall(output, test_classes))\n",
    "    confusion.append( confusion_matrix(output, test_classes))\n",
    "\n",
    "print accuracies\n",
    "print precisions\n",
    "print recalls\n",
    "print confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Part 3: Random Forests\n",
    "-------\n",
    "30 pts.\n",
    "\n",
    "<ul><li>File to use: part23_data.csv</li>\n",
    "\n",
    "<li>Grading: average test accuracy over 10 rounds should be >= 75%</li></ul>\n",
    "\n",
    "\n",
    "The decision boundaries drawn by decision trees are very sharp, and fitting a decision tree of unbounded depth to a list of training examples almost inevitably leads to overfitting. In an attempt to decrease the variance of our classifier we're going to use a technique called 'Bootstrap Aggregating' (often abbreviated 'bagging').\n",
    "\n",
    "A Random Forest is a collection of decision trees, build as follows:\n",
    "\n",
    "    1) For every tree we're going to build:\n",
    "\n",
    "        a) Subsample the examples provided us (with replacement) in accordance with a provided example subsampling rate.\n",
    "\n",
    "        b) From the sample in a), choose attributes at random to learn on (in accordance with a provided attribute subsampling rate)\n",
    "\n",
    "        c) Fit a decision tree to the subsample of data we've chosen (to a certain depth)\n",
    "    \n",
    "Classification for a random forest is then done by taking a majority vote of the classifications yielded by each tree in the forest after it classifies an example.\n",
    "\n",
    "Fill in `RandomForest.fit()` to fit the decision tree as we describe above, and fill in `RandomForest.classify()` to classify a given list of examples.\n",
    "\n",
    "Your features and classify should be in numpy arrays where if the dataset was (m x n) then the features is (m x n-1) and classify is (n x 1).\n",
    "\n",
    "To test, we will be using a forest with 5 trees, with a depth limit of 5, example subsample rate of 0.5 and attribute subsample rate of 0.5\n",
    "\n",
    "How grading works: similar to 2b but with the call to Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    \"\"\"Class for random forest\n",
    "    classification.\"\"\"\n",
    "\n",
    "    def __init__(self, num_trees, depth_limit, example_subsample_rate, attr_subsample_rate):\n",
    "        \"\"\"Create a random forest with a fixed \n",
    "        number of trees, depth limit, example\n",
    "        sub-sample rate and attribute sub-sample\n",
    "        rate.\"\"\"\n",
    "        self.trees = []\n",
    "        self.num_trees = num_trees\n",
    "        self.depth_limit = depth_limit\n",
    "        self.example_subsample_rate = example_subsample_rate\n",
    "        self.attr_subsample_rate = attr_subsample_rate\n",
    "\n",
    "    def fit(self, features, classes):\n",
    "        \"\"\"Build a random forest of \n",
    "        decision trees.\"\"\"\n",
    "        # TODO implement the above algorithm\n",
    "        sample_size, attr_size = features.shape\n",
    "        sample_indices = np.arange(sample_size)\n",
    "        attr_indices = np.arange(attr_size)\n",
    "        subsample_size = np.int(sample_size * self.example_subsample_rate)\n",
    "        subattr_size = np.int(attr_size * self.attr_subsample_rate)\n",
    "\n",
    "        for i in xrange(self.num_trees):\n",
    "            subsample_indices = np.random.choice(sample_indices, subsample_size, replace=False)\n",
    "            subattr_indices = np.sort(np.random.choice(attr_indices, subattr_size, replace=False))\n",
    "\n",
    "            subsample_features = features[subsample_indices][:, subattr_indices]\n",
    "            subsample_classes = classes[subsample_indices]\n",
    "\n",
    "            tree = DecisionTree(self.depth_limit)\n",
    "            tree.fit(subsample_features, subsample_classes)\n",
    "\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def classify(self, features):\n",
    "        \"\"\"Classify a list of features based\n",
    "        on the trained random forest.\"\"\"\n",
    "        # TODO implement classification for a random forest.\n",
    "        classes = np.zeros(features.shape[0])\n",
    "        for tree in self.trees:\n",
    "            classes = np.add(classes, tree.classify(features))\n",
    "        threshold = len(self.trees) / 2.\n",
    "        indices = classes >= threshold\n",
    "        classes[:] = 0\n",
    "        classes[indices] = 1\n",
    "        return classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Part 4: Challenge Classifier\n",
    "-------\n",
    "\n",
    "10 points.\n",
    "\n",
    "<ul><li>File to use: challenge_train.csv</li>\n",
    "\n",
    "<li>Grading: average training accuracy over 10 runs should be >= 80% and average ru accuracy over 10 runs should be >= 70%</li></ul>\n",
    "\n",
    "You should be implementing some sort of a <b>tree structure</b>, students in the past have been able to call their RandomForest with different parameters. We also encourage things like boosting.\n",
    "\n",
    "You've been provided with a sample of data from a research dataset in 'challenge_train.csv' while we have reserved a part of the dataset for testing called challenge_test.csv (which you do not have access to). \n",
    "\n",
    "To get full points for this part of the assignment, you'll need to get at least an average accuracy of 80% on the training data you have (challenge_train.csv), and at least an average accuracy of 70% on the holdout/test set (challenge_test.csv). We do provide how long it takes for your training and testing to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class ChallengeClassifier():\n",
    "    \n",
    "    def __init__(self):\n",
    "        # initialize whatever parameters you may need here-\n",
    "        # this method will be called without parameters \n",
    "        # so if you add any to make parameter sweeps easier, provide defaults\n",
    "        self.tree = RandomForest(10, 10, 0.25, 1)\n",
    "        \n",
    "    def fit(self, features, classes):\n",
    "        # fit your model to the provided features\n",
    "        self.tree.fit(features, classes)\n",
    "        \n",
    "    def classify(self, features):\n",
    "        # classify each feature in features as either 0 or 1.\n",
    "        return self.tree.classify(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "challenge_dataset = load_csv('challenge_train.csv', 0)\n",
    "ten_folds = generate_k_folds(challenge_dataset, 10)\n",
    "\n",
    "challenge_features, challenge_classes = challenge_dataset\n",
    "challenge_classes = np.array(challenge_classes)\n",
    "\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "confusion = []\n",
    "\n",
    "for fold in ten_folds:\n",
    "    train, test = fold\n",
    "    train_features, train_classes = train\n",
    "    test_features, test_classes = test\n",
    "    tree = ChallengeClassifier( )\n",
    "    tree.fit( train_features, train_classes)\n",
    "    output = tree.classify(test_features)\n",
    "    \n",
    "    accuracies.append( accuracy(output, test_classes))\n",
    "    precisions.append( precision(output, test_classes))\n",
    "    recalls.append( recall(output, test_classes))\n",
    "    confusion.append( confusion_matrix(output, test_classes))\n",
    "\n",
    "print accuracies\n",
    "print precisions\n",
    "print recalls\n",
    "print confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Part 5: Vectorization!\n",
    "-------\n",
    "\n",
    "7 points.\n",
    "\n",
    "File to use: vectorize.csv\n",
    "    \n",
    "Last semester, students struggled a lot with assignment 5 not because of the assignment but of the vectorization requirement so that it can run under the time limit we have. As a result, we are adding a small section to this assignment that will hopefully introduce you to vectorization and some of the cool tricks you can use in python. We encourage you to use any numpy function out there (on good faith) to do the following functions. \n",
    "\n",
    "For the three functions that we have, we are testing your code based on how fast it runs. It will need to beat the non-vectorized code to get full points.\n",
    "\n",
    "As a reminder, please don't ask the TA's for help regarding this section, we will not be able to assist you in any way. This section was created to help get you ready for assignment_5; feel free to ask other students on Piazza or use the Internet.\n",
    "\n",
    "How grading works: we run the non-vectorized code and your vectorized code 500 times, as long as the average time of your vectorized code is less than the average time of the non-vectorized code, you will get the points (given that your answer is correct).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import resource\n",
    "import numpy as np\n",
    "\n",
    "class Vectorization():\n",
    "    \n",
    "    def load_csv(self,data_file_path, class_index):\n",
    "        handle = open(data_file_path, 'r')\n",
    "        contents = handle.read()\n",
    "        handle.close()\n",
    "        rows = contents.split('\\n')\n",
    "        out = np.array([[float(i) for i in r.split(',')] for r in rows if r])\n",
    "\n",
    "        if(class_index == -1):\n",
    "            classes= map(int,  out[:,class_index])\n",
    "            features = out[:,:class_index]\n",
    "            return features, classes\n",
    "        elif(class_index == 0):\n",
    "            classes= map(int,  out[:, class_index])\n",
    "            features = out[:, 1:]\n",
    "            return features, classes\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "    # Vectorization #1: Loops!\n",
    "    # This function takes one matrix, multiplies by itself and then adds to itself.\n",
    "    # Output: return a numpy array\n",
    "    # 1 point\n",
    "    def non_vectorized_loops(self, data):\n",
    "        non_vectorized = np.zeros(data.shape)\n",
    "        for row in range(data.shape[0]):\n",
    "            for col in range(data.shape[1]):\n",
    "                non_vectorized[row][col] = data[row][col] * data[row][col] + data[row][col]\n",
    "        return non_vectorized\n",
    "\n",
    "    def vectorized_loops(self, data):\n",
    "        # TODO vectorize the code from above\n",
    "        # Bonnie time to beat: 0.09 seconds\n",
    "        return data * data + data\n",
    "        \n",
    "    def vectorize_1(self):\n",
    "        data = self.load_csv('vectorize.csv', 1)\n",
    "        start_time = resource.getrusage(resource.RUSAGE_SELF).ru_utime * 1000\n",
    "        real_answer = self.non_vectorized_loops(data)\n",
    "        end_time = resource.getrusage(resource.RUSAGE_SELF).ru_utime * 1000\n",
    "        print 'Non-vectorized code took %s seconds' % str(end_time-start_time)\n",
    "\n",
    "        start_time = resource.getrusage(resource.RUSAGE_SELF).ru_utime * 1000\n",
    "        my_answer = self.vectorized_loops(data)\n",
    "        end_time = resource.getrusage(resource.RUSAGE_SELF).ru_utime * 1000\n",
    "        print 'Vectorized code took %s seconds' % str(end_time-start_time)\n",
    "        \n",
    "        assert np.array_equal(real_answer, my_answer), \"TEST FAILED\"\n",
    "    \n",
    "    # Vectorization #2: Slicing and summation\n",
    "    # This function searches through the first 100 rows, looking for the row with the max sum \n",
    "    # (ie, add all the values in that row together)\n",
    "    # Output: return the max sum as well as the row number for the max sum\n",
    "    # 3 points\n",
    "    def non_vectorized_slice(self, data):\n",
    "        max_sum = 0\n",
    "        max_sum_index = 0\n",
    "        for row in range(100):\n",
    "            temp_sum = 0\n",
    "            for col in range(data.shape[1]):\n",
    "                temp_sum += data[row][col]\n",
    "\n",
    "            if (temp_sum > max_sum):\n",
    "                max_sum = temp_sum\n",
    "                max_sum_index = row\n",
    "\n",
    "        return max_sum, max_sum_index\n",
    "\n",
    "    def vectorized_slice(self, data):\n",
    "        # TODO vectorize the code from above\n",
    "        # Bonnie time to beat: 0.07 seconds\n",
    "        sum_list = np.sum(data[:100], axis=1)\n",
    "        max_sum_index = np.argmax(sum_list)\n",
    "        max_sum = sum_list[max_sum_index]\n",
    "        return max_sum, max_sum_index\n",
    "        \n",
    "    def vectorize_2(self):\n",
    "        data = self.load_csv('vectorize.csv', 1)\n",
    "        start_time = resource.getrusage(resource.RUSAGE_SELF).ru_utime * 1000\n",
    "        real_sum, real_sum_index = self.non_vectorized_slice(data)\n",
    "        end_time = resource.getrusage(resource.RUSAGE_SELF).ru_utime * 1000\n",
    "        print 'Non-vectorized code took %s seconds' % str(end_time-start_time)\n",
    "\n",
    "        start_time = resource.getrusage(resource.RUSAGE_SELF).ru_utime * 1000\n",
    "        my_sum, my_sum_index = self.vectorized_slice(data)\n",
    "        end_time = resource.getrusage(resource.RUSAGE_SELF).ru_utime * 1000\n",
    "        print 'Vectorized code took %s seconds' % str(end_time-start_time)\n",
    "\n",
    "        assert (real_sum==my_sum),\"TEST FAILED\"\n",
    "        assert (real_sum_index==my_sum_index), \"TEST FAILED\"\n",
    "        \n",
    "    # Vectorization #3: Flattening and dictionaries \n",
    "    # This function flattens down data into a 1d array, creates a dictionary of how often a  \n",
    "    # positive number appears in the data and displays that value\n",
    "    # Output: list of tuples [(1203,3)] = 1203 appeared 3 times in data\n",
    "    # 3 points\n",
    "    def non_vectorized_flatten(self, data):\n",
    "        unique_dict = {}\n",
    "        flattened = np.hstack(data)\n",
    "        for item in range(len(flattened)):\n",
    "            if flattened[item] > 0:\n",
    "                if flattened[item] in unique_dict:\n",
    "                    unique_dict[flattened[item]] += 1\n",
    "                else:\n",
    "                    unique_dict[flattened[item]] = 1\n",
    "\n",
    "        return unique_dict.items()\n",
    "\n",
    "    def vectorized_flatten(self, data):\n",
    "        # TODO vectorize the code from above\n",
    "        # Bonnie time to beat: 15 seconds\n",
    "        flattened = data[data > 0.]\n",
    "        unique_dict = Counter(flattened)\n",
    "        return unique_dict.items()\n",
    "\n",
    "    def vectorize_3(self):\n",
    "        data = self.load_csv('vectorize.csv', 1)\n",
    "        start_time = resource.getrusage(resource.RUSAGE_SELF).ru_utime * 1000\n",
    "        answer_unique = self.non_vectorized_flatten(data)\n",
    "        end_time = resource.getrusage(resource.RUSAGE_SELF).ru_utime * 1000\n",
    "        print 'Non-vectorized code took %s seconds'% str(end_time-start_time)\n",
    "\n",
    "        start_time = resource.getrusage(resource.RUSAGE_SELF).ru_utime * 1000\n",
    "        my_unique = self.vectorized_flatten(data)\n",
    "        end_time = resource.getrusage(resource.RUSAGE_SELF).ru_utime * 1000\n",
    "        print 'Vectorized code took %s seconds'% str(end_time-start_time)\n",
    "\n",
    "        assert np.array_equal(answer_unique, my_unique), \"TEST FAILED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vectorize = Vectorization()\n",
    "\n",
    "vectorize.vectorize_1()\n",
    "vectorize.vectorize_2()\n",
    "vectorize.vectorize_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Bonus\n",
    "----\n",
    "\n",
    "Note: this part will be changing. Official annoucements for this bonus will be made through Piazza\n",
    "\n",
    "We will be having a competition using your challenge classifier and a dataset of our choice. We will provide you with a portion of the dataset as well as the testing data (but without the labels) and you will upload your solution as a csv to Kaggle. Kaggle will evaluate your scores and the classifier with the highest accuracy will win the competiton. Any ties will be broken by the submission time. \n",
    "\n",
    "We are still figuring out all the details for this bonus so hopefully it will be out by the time the midterm period is over. We will keep the competition available for at least a few weeks. \n",
    "\n",
    "<ul>\n",
    "    <li>First place:  3 bonus points on your final grade </li>\n",
    "    <li>Second place: 2 bonus points on your final grade </li>\n",
    "    <li>Third place:  1 bonus point on your final grade </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
