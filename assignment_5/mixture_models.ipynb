{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Assignment 5: Gaussian mixture models\n",
    "\n",
    "## The Challenge\n",
    "\n",
    "Automatic image processing is a key component to many AI systems, including facial recognition and video compression. One basic method for processing is segmentation, by which we divide an image into a fixed number of components in order to simplify its representation. For example, we can train a mixture of Gaussians to represent an image, and segment it according to the simplified representation as shown in the images below.\n",
    "\n",
    "In this assignment, you will learn to perform image segmentation. To this end, you will implement Gaussian mixture models and iteratively improve their performance. You will perform this segmentation on the \"Bird\" and \"Party Spock\" images included with the assignment.\n",
    "\n",
    "## About the Assignment\n",
    "\n",
    "The tests for the assignment are provided in the notebook, so Bonnie is only for submission purposes. The tests on Bonnie will be similar to the ones provided here, but the images being tested against, and the values for calculations will be different.\n",
    "\n",
    "Thus, you will be allowed only **5 submissions** on Bonnie. Make sure you test **everything** before submitting. Score for the last submission counts. The code will be allowed to run for not more than 2 hours per submission. In order for the code to run quickly, make sure to vectorize the code (more on this below).\n",
    "\n",
    "## Your Mission (should you choose to accept it)\n",
    "\n",
    "Your assignment is to implement several methods of image segmentation, with increasing complexity.\n",
    "1. Implement k-means clustering to segment a color image.\n",
    "2. Build a Gaussian mixture model to be trained with expectation-maximization.\n",
    "3. Experiment with varying the details of the Gaussian mixture modelâ€™s implementation.\n",
    "4. Implement and test a new metric called the Bayesian information criterion, which guarantees a more robust image segmentation.\n",
    "\n",
    "## Grading\n",
    "\n",
    "The grade you receive for the assignment will be distributed as follows:\n",
    "1. k-Means Clustering (20 points)\n",
    "2. Gaussian Mixture Model (40 points)\n",
    "3. Model Performance Improvements (20 points)\n",
    "4. Bayesian Information Criterion (20 points)\n",
    "5. Bonus\n",
    "\n",
    "## Due Date\n",
    "\n",
    "The assignment is due **April 2nd, 2017 at 11:59PM UTC-12 (Anywhere on Earth time)**. The deliverable for this assignment is a completed mixture_models.py file.\n",
    "\n",
    "## Resources\n",
    "\n",
    "The em.pdf chapter in the assignment folder gives a good explanation of implementing and training mixture models, particularly 424-427 (k-means) and 435-439 (mixture models and EM).\n",
    "The book [_Elements of Statistical Learning_](http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf), pages 291-295.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Background\n",
    "\n",
    "A Gaussian mixture model is a generative model for representing the underlying probability distribution of a complex collection of data, such as the collection of pixels in a grayscale photograph. \n",
    "\n",
    "In the context of this problem, a Gaussian mixture model defines the joint probability $f(x)$ as\n",
    "\n",
    "$$f(x) = \\sum_{i=1}^{k}m_{i}N_i(x |  \\mu_{i}, \\sigma_{i}^{2})$$\n",
    "\n",
    "where $x$ is a grayscale value [0,1], $f(x)$ is the joint probability of that gray scale value, $m_{i}$ is the mixing coefficient on component $i$, $N_i$ is the $i^{th}$ Gaussian distribution underlying the value $x$ with mean $\\mu_{i}$ and variance $\\sigma_{i}^{2}$. \n",
    "\n",
    "We will be using this model to segment photographs into different grayscale regions. The idea of segmentation is to assign a component $i$ to each pixel $x$ using the maximum posterior probability\n",
    "\n",
    "$$component_{x} = argmax_{i}(m_{i}N_i(x|\\mu_{i},\\sigma_{i}^{2})$$\n",
    "\n",
    "Then we will replace each pixel in the image with its corresponding $\\mu_{i}$ to produce a result as below (original above, segmented with three components below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![Original image.](images/party_spock.png)\n",
    "![Post-segmentation.](images/party_spock3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import warnings\n",
    "warnings.simplefilter(action = \"ignore\", category = FutureWarning)\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from matplotlib import image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Helper image-processing code. \n",
    "These have been added in a separate python file and added in to the repo.\n",
    "The functions below have all been imported in to your submission file\"\"\"\n",
    "def image_to_matrix(image_file, grays=False):\n",
    "    \"\"\"\n",
    "    Convert .png image to matrix\n",
    "    of values.\n",
    "    \n",
    "    params:\n",
    "    image_file = str\n",
    "    grays = Boolean\n",
    "    \n",
    "    returns:\n",
    "    img = (color) np.ndarray[np.ndarray[np.ndarray[float]]] \n",
    "    or (grayscale) np.ndarray[np.ndarray[float]]\n",
    "    \"\"\"\n",
    "    img = image.imread(image_file)\n",
    "    # in case of transparency values\n",
    "    if(len(img.shape) == 3 and img.shape[2] > 3):\n",
    "        height, width, depth = img.shape\n",
    "        new_img = np.zeros([height, width, 3])\n",
    "        for r in range(height):\n",
    "            for c in range(width):\n",
    "                new_img[r,c,:] = img[r,c,0:3]\n",
    "        img = np.copy(new_img)\n",
    "    if(grays and len(img.shape) == 3):\n",
    "        height, width = img.shape[0:2]\n",
    "        new_img = np.zeros([height, width])\n",
    "        for r in range(height):\n",
    "            for c in range(width):\n",
    "                new_img[r,c] = img[r,c,0]\n",
    "        img = new_img\n",
    "    return img\n",
    "\n",
    "def matrix_to_image(image_matrix, image_file):\n",
    "    \"\"\"\n",
    "    Convert matrix of color/grayscale \n",
    "    values  to .png image\n",
    "    and save to file.\n",
    "    \n",
    "    params:\n",
    "    image_matrix = (color) numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] or (grayscale) numpy.ndarray[numpy.ndarray[float]]\n",
    "    image_file = str\n",
    "    \"\"\"\n",
    "    # provide cmap to grayscale images\n",
    "    cMap = None\n",
    "    if(len(image_matrix.shape) < 3):\n",
    "        cMap = cm.Greys_r\n",
    "    image.imsave(image_file, image_matrix, cmap=cMap)\n",
    "    \n",
    "def flatten_image_matrix(image_matrix):\n",
    "    \"\"\"\n",
    "    Flatten image matrix from \n",
    "    Height by Width by Depth\n",
    "    to (Height*Width) by Depth\n",
    "    matrix.\n",
    "    \n",
    "    params:\n",
    "    image_matrix = (color) numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] or (grayscale) numpy.ndarray[numpy.ndarray[float]]\n",
    "    \n",
    "    returns:\n",
    "    flattened_values = (color) numpy.ndarray[numpy.ndarray[float]] or (grayscale) numpy.ndarray[float]    \n",
    "    \"\"\"\n",
    "    if(len(image_matrix.shape) == 3):\n",
    "        height, width, depth = image_matrix.shape\n",
    "    else:\n",
    "        height, width = image_matrix.shape\n",
    "        depth = 1\n",
    "    return image_matrix.reshape(height*width, depth)\n",
    "\n",
    "def unflatten_image_matrix(image_matrix, width):\n",
    "    \"\"\"\n",
    "    Unflatten image matrix from\n",
    "    (Height*Width) by Depth to\n",
    "    Height by Width by Depth matrix.\n",
    "    \n",
    "    params:\n",
    "    image_matrix = (color) numpy.ndarray[numpy.ndarray[float]] or (grayscale) numpy.ndarray[float]\n",
    "    width = int\n",
    "    \n",
    "    returns:\n",
    "    unflattened_values = (color) numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] or (grayscale) numpy.ndarray[numpy.ndarray[float]]\n",
    "    \"\"\"\n",
    "    heightWidth = image_matrix.shape[0]\n",
    "    height = int(heightWidth / width)\n",
    "    if(len(image_matrix.shape) > 1 and image_matrix.shape[-1] != 1):\n",
    "        depth = image_matrix.shape[-1]\n",
    "        return image_matrix.reshape(height, width, depth)\n",
    "    else:\n",
    "        return image_matrix.reshape(height, width)\n",
    "\n",
    "def image_difference(image_values_1, image_values_2):\n",
    "    \"\"\"\n",
    "    Calculate the total difference \n",
    "    in values between two images.\n",
    "    Assumes that both images have same\n",
    "    shape.\n",
    "    \n",
    "    params:\n",
    "    image_values_1 = (color) numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] or (grayscale) numpy.ndarray[numpy.ndarray[float]]\n",
    "    image_values_2 = (color) numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] or (grayscale) numpy.ndarray[numpy.ndarray[float]]\n",
    "    \n",
    "    returns:\n",
    "    dist = int\n",
    "    \"\"\"\n",
    "    flat_vals_1 = flatten_image_matrix(image_values_1)\n",
    "    flat_vals_2 = flatten_image_matrix(image_values_2)\n",
    "    N, depth = flat_vals_1.shape\n",
    "    dist = 0.\n",
    "    point_thresh = 0.005\n",
    "    for i in range(N):\n",
    "        if(depth > 1):\n",
    "            new_dist = sum(abs(flat_vals_1[i] - flat_vals_2[i]))\n",
    "            if(new_dist > depth * point_thresh):\n",
    "                dist += new_dist\n",
    "        else:\n",
    "            new_dist = abs(flat_vals_1[i] - flat_vals_2[i])\n",
    "            if(new_dist > point_thresh):\n",
    "                dist += new_dist\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "image_dir = 'images/'\n",
    "image_file = 'party_spock.png'\n",
    "values = image_to_matrix(image_dir + image_file)\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Part 0: Note on Vectorization\n",
    "--\n",
    "The concept of Vectorization was introduced in the last section of Assignment 4. For this assignment, please vectorize your code wherever possible using numpy arrays, instead of running for-loops over the images being processed.\n",
    "\n",
    "For an example of how this might be useful, consider the following array:\n",
    "\n",
    "A = [12 34 1234 764 ...(has a million values)... 91, 78]\n",
    "\n",
    "Now you need to calculate another array B, which has the same dimensions as A above.\n",
    "Say each value in B is calculated as follows:\n",
    "\n",
    "(each value in B) = square_root_of(some constants * pi * log(k) * (each value in A))/7\n",
    "\n",
    "You might wish to use a for-loop to compute this. However, it will take really long to run on an array of this magnitude. \n",
    "\n",
    "Alternatively, you may choose to use numpy and perform this calculation in a single line. You can pass A as a numpy array and the entire calculation will be done in a line, resulting in B being populated with the corresponding values that come out of this formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Part 1: K-means clustering\n",
    "--\n",
    "20 pts\n",
    "\n",
    "One easy method for image segmentation is to simply cluster all similar data points together and then replace their values with the mean value. Thus, we'll warm up using k-means clustering. This will also provide a baseline to compare with your segmentation. Please note that clustering will come in handy later. \n",
    "\n",
    "Fill out `k_means_cluster()` to convert the original image values matrix to its clustered counterpart. Your convergence test should be whether the assigned clusters stop changing. Note that this convergence test is rather slow. When no initial cluster means are provided, `k_means_cluster()` should choose $k$ random points from the data (without replacement) to use as initial cluster means.\n",
    "\n",
    "For this part of the assignment, since clustering is best used on multidimensional data, we will be using the color image `bird_color_24.png`.\n",
    "\n",
    "You can test your implementation of k-means using our reference images in `k_means_test()`.\n",
    "\n",
    "#### Try to vectorize the code for it to run faster. Without vectorization it takes 25-30 minutes for the code to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from functools import reduce\n",
    "def k_means_cluster(image_values, k=3, initial_means=None):\n",
    "    \"\"\"\n",
    "    Separate the provided RGB values into\n",
    "    k separate clusters using the k-means algorithm,\n",
    "    then return an updated version of the image\n",
    "    with the original values replaced with\n",
    "    the corresponding cluster values.\n",
    "    \n",
    "    params:\n",
    "    image_values = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]]\n",
    "    k = int\n",
    "    initial_means = numpy.ndarray[numpy.ndarray[float]] or None\n",
    "    \n",
    "    returns:\n",
    "    updated_image_values = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]]\n",
    "    \"\"\"\n",
    "    # TODO: finish this function\n",
    "    raise NotImplementedError()\n",
    "    return updated_image_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def k_means_test():\n",
    "    \"\"\"\n",
    "    Testing your implementation\n",
    "    of k-means on the segmented\n",
    "    bird_color_24 reference images.\n",
    "    \"\"\"\n",
    "    k_min = 2\n",
    "    k_max = 6\n",
    "    image_dir = 'images/'\n",
    "    image_name = 'bird_color_24.png'\n",
    "    image_values = image_to_matrix(image_dir + image_name)\n",
    "    # initial mean for each k value\n",
    "    initial_means = [\n",
    "        np.array([[0.90980393,0.8392157,0.65098041],[0.83137256,0.80784315,0.69411767]]),\n",
    "        np.array([[0.90980393,0.8392157,0.65098041],[0.83137256,0.80784315,0.69411767],[0.67450982,0.52941179,0.25490198]]),\n",
    "        np.array([[0.90980393,0.8392157,0.65098041],[0.83137256,0.80784315,0.69411767],[0.67450982,0.52941179,0.25490198],[0.86666667,0.8392157,0.70588237]]),\n",
    "        np.array([[0.90980393,0.8392157,0.65098041],[0.83137256,0.80784315,0.69411767],[0.67450982,0.52941179,0.25490198],[0.86666667,0.8392157,0.70588237],[0,0,0]]),\n",
    "        np.array([[0.90980393,0.8392157,0.65098041],[0.83137256,0.80784315,0.69411767],[0.67450982,0.52941179,0.25490198],[0.86666667,0.8392157,0.70588237],[0,0,0],[0.8392157,0.80392158,0.63921571]]),\n",
    "    ]\n",
    "    # test different k values to find best\n",
    "    for k in range(k_min, k_max+1):\n",
    "        updated_values = k_means_cluster(image_values, k, initial_means[k-k_min])\n",
    "        ref_image = image_dir + 'k%d_%s'%(k, image_name)\n",
    "        ref_values = image_to_matrix(ref_image)\n",
    "        dist = image_difference(updated_values, ref_values)\n",
    "        print('Image distance = %.2f'%(dist))\n",
    "        if(int(dist) == 0):\n",
    "            print('Clustering for %d clusters produced a realistic image segmentation.'%(k))\n",
    "        else:\n",
    "            print('Clustering for %d clusters didn\\'t produce a realistic image segmentation.'%(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "k_means_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Part 2: Implementing a Gaussian mixture model\n",
    "--\n",
    "40 pts\n",
    "\n",
    "Next, we will step beyond clustering and implement a complete Gaussian mixture model. \n",
    "\n",
    "Complete the below implementation of `GaussianMixtureModel` so that it can perform the following:\n",
    "\n",
    "1. Calculate the joint log probability of a given greyscale value. (5 points)\n",
    "2. Use expectation-maximization (EM) to train the model to represent the image as a mixture of Gaussians. (20 points) To initialize EM, set each component's mean to the grayscale value of randomly chosen pixel and variance to 1, and the mixing coefficients to a uniform distribution. Note: there are packages that can run EM automagically, but please implement your own version of EM without using these extra packages. We've set the convergence condition for you in `GaussianMixtureModel.default_convergence()`: if the new likelihood is within 10% of the previous likelihood for 10 consecutive iterations, the model has converged.\n",
    "3. Calculate the log likelihood of the trained model. (5 points)\n",
    "4. Segment the image according to the trained model. (5 points)\n",
    "5. Determine the best segmentation by iterating over model training and scoring, since EM isn't guaranteed to converge to the global maximum. (5 points)\n",
    "\n",
    "We have provided the necessary tests for this part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### When multiplying lots of probabilities in sequence, you can end up with a probability of zero due to underflow. To avoid this, you should calculate the log probabilities for the entire assignment. \n",
    "\n",
    "The log form of the Gaussian probability of scalar value $x$ is:\n",
    "\n",
    "$$ln (N(x | \\mu, \\sigma)) = -0.5ln(2\\pi \\sigma^2) - \\frac{(x-\\mu)^{2}}{2\\sigma^2} $$\n",
    "\n",
    "where $\\mu$ is the mean and $\\sigma$ is standard deviation.\n",
    "\n",
    "You can calculate the sum of log probabilities by using `scipy.misc.logsumexp()`. For example, `logsumexp([-2,-3])` will return the same result as `numpy.log(numpy.exp(-2)+numpy.exp(-3))`.\n",
    "\n",
    "In other words, `logsumexp(a, b) = log(e^a + e^b)`.\n",
    "\n",
    "#### Rather than using lists of lists, you will find it much easier to store your data in `numpy.array` arrays.\n",
    "\n",
    "You can instantiate them using the command\n",
    "\n",
    "    matrix = numpy.zeros([rows, columns])\n",
    "\n",
    "where `rows` is the number of rows and `columns` is the number of columns in your matrix. `numpy.zeros()` generates a matrix of the specified size containing 0s at each row/column cell. You can access cells with the syntax `matrix[2,3]` which will return the value in row 2 and column 3.\n",
    "\n",
    "### Warning: You may lose all marks for this part if your code runs for too long.\n",
    "### You will need to vectorize your code in this part. Specifically, the method train_model() needs to perform operations using numpy arrays, as does likelihood(), which calculates the log likelihood. These are time-sensitive operations and will be called over and over as you proceed with this assignment.\n",
    "\n",
    "For the assignment, focus on vectorizing the following: \n",
    "1. The calculations for the Expectation step, where you calculate joint probabilities \n",
    "2. The calculations where you update your means, variances and mixing coefficients in the Maximization step. \n",
    "\n",
    "Remember, these are fundamental operations and will be called a lot in the remainder of the assignment. So it is crucial you optimize these.\n",
    "\n",
    "For the synthetic data test which we provide to check if your training is working, the set is too small and it won't make a difference. But with the actual image that we use ahead, for-loops won't do good. Vectorized code would take under 30 seconds to converge which would typically involve about 15-20 iterations with the convergence function we have here.\n",
    "Inefficient code that uses loops or iterates over each pixel value sequentially, will take hours to run. You don't want to do that because:\n",
    "\n",
    "1. You won't have that much time to test to your code.\n",
    "2. You won't be getting marks. We will be capping the run time and kill anything that takes over 2 minutes for each iteration.\n",
    "\n",
    "#### You will want to have your image pixel values as a one-dimensional array to perform these operations. We have provided a method to flatten the image for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def default_convergence(prev_likelihood, new_likelihood, conv_ctr, conv_ctr_cap=10):\n",
    "    \"\"\"\n",
    "    Default condition for increasing\n",
    "    convergence counter: \n",
    "    new likelihood deviates less than 10%\n",
    "    from previous likelihood.\n",
    "\n",
    "    params:\n",
    "    prev_likelihood = float\n",
    "    new_likelihood = float\n",
    "    conv_ctr = int\n",
    "    conv_ctr_cap = int\n",
    "\n",
    "    returns:\n",
    "    conv_ctr = int\n",
    "    converged = boolean\n",
    "    \"\"\"\n",
    "    increase_convergence_ctr = (abs(prev_likelihood) * 0.9 < \n",
    "                                abs(new_likelihood) < \n",
    "                                abs(prev_likelihood) * 1.1)\n",
    "    \n",
    "    if increase_convergence_ctr:\n",
    "        conv_ctr+=1\n",
    "    else:\n",
    "        conv_ctr =0\n",
    "        \n",
    "    return conv_ctr, conv_ctr > conv_ctr_cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import math\n",
    "from scipy.misc import logsumexp\n",
    "class GaussianMixtureModel:\n",
    "    \"\"\"\n",
    "    A Gaussian mixture model\n",
    "    to represent a provided \n",
    "    grayscale image.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, image_matrix, num_components, means=None):\n",
    "        \"\"\"\n",
    "        Initialize a Gaussian mixture model.\n",
    "        \n",
    "        params:\n",
    "        image_matrix = (grayscale) numpy.nparray[numpy.nparray[float]]\n",
    "        num_components = int\n",
    "        \"\"\"\n",
    "        self.image_matrix = image_matrix\n",
    "        self.num_components = num_components\n",
    "        if(means is None):\n",
    "            self.means = [0]*num_components\n",
    "        else:\n",
    "            self.means = means\n",
    "        self.variances = [0]*num_components\n",
    "        self.mixing_coefficients = [0]*num_components\n",
    "    \n",
    "    def joint_prob(self, val):\n",
    "        \"\"\"Calculate the joint \n",
    "        log probability of a greyscale\n",
    "        value within the image.\n",
    "        \n",
    "        params:\n",
    "        val = float\n",
    "        \n",
    "        returns:\n",
    "        joint_prob = float\n",
    "        \"\"\"\n",
    "        # TODO: finish this\n",
    "        raise NotImplementedError()\n",
    "        return joint_prob\n",
    "    \n",
    "    def initialize_training(self):\n",
    "        \"\"\"\n",
    "        Initialize the training\n",
    "        process by setting each\n",
    "        component mean to a random\n",
    "        pixel's value (without replacement),\n",
    "        each component variance to 1, and\n",
    "        each component mixing coefficient\n",
    "        to a uniform value \n",
    "        (e.g. 4 components -> [0.25,0.25,0.25,0.25]).\n",
    "        \n",
    "        NOTE: this should be called before \n",
    "        train_model() in order for tests\n",
    "        to execute correctly.\n",
    "        \"\"\"\n",
    "        # TODO: finish this\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def train_model(self, convergence_function=default_convergence):\n",
    "        \"\"\"\n",
    "        Train the mixture model \n",
    "        using the expectation-maximization\n",
    "        algorithm. Since each Gaussian is\n",
    "        a combination of mean and variance,\n",
    "        this will fill self.means and \n",
    "        self.variances, plus \n",
    "        self.mixing_coefficients, with\n",
    "        the values that maximize\n",
    "        the overall model likelihood.\n",
    "        \n",
    "        params:\n",
    "        convergence_function = function that returns True if convergence is reached\n",
    "        \"\"\"\n",
    "        # TODO: finish this\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def segment(self):\n",
    "        \"\"\"\n",
    "        Using the trained model, \n",
    "        segment the image matrix into\n",
    "        the pre-specified number of \n",
    "        components. Returns the original \n",
    "        image matrix with the each \n",
    "        pixel's intensity replaced \n",
    "        with its max-likelihood \n",
    "        component mean.\n",
    "        \n",
    "        returns:\n",
    "        segment = numpy.ndarray[numpy.ndarray[float]]\n",
    "        \"\"\"\n",
    "        # TODO: finish this\n",
    "        raise NotImplementedError()\n",
    "        return segment\n",
    "    \n",
    "    def likelihood(self):\n",
    "        \"\"\"Assign a log \n",
    "        likelihood to the trained\n",
    "        model based on the following \n",
    "        formula for posterior probability:\n",
    "        ln(Pr(X | mixing, mean, stdev)) = sum((n=1 to N),ln(sum((k=1 to K), mixing_k * N(x_n | mean_k, stdev_k) )))\n",
    "        \n",
    "        returns:\n",
    "        log_likelihood = float \n",
    "        \"\"\"\n",
    "        # TODO: finish this\n",
    "        raise NotImplementedError()\n",
    "        return log_likelihood\n",
    "        \n",
    "    def best_segment(self, iters):\n",
    "        \"\"\"Determine the best segmentation\n",
    "        of the image by repeatedly \n",
    "        training the model and \n",
    "        calculating its likelihood. \n",
    "        Return the segment with the\n",
    "        highest likelihood.\n",
    "        \n",
    "        params:\n",
    "        iters = int\n",
    "        \n",
    "        returns:\n",
    "        segment = numpy.ndarray[numpy.ndarray[float]]\n",
    "        \"\"\"\n",
    "        # finish this\n",
    "        raise NotImplementedError()\n",
    "        return segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def gmm_likelihood_test():\n",
    "    \"\"\"Testing the GMM method\n",
    "    for calculating the overall\n",
    "    model probability.\n",
    "    Should return -364370.\n",
    "    \n",
    "    returns:\n",
    "    likelihood = float\n",
    "    \"\"\"\n",
    "    image_file = 'images/party_spock.png'\n",
    "    image_matrix = image_to_matrix(image_file)\n",
    "    num_components = 5\n",
    "    gmm = GaussianMixtureModel(image_matrix, num_components)\n",
    "    gmm.initialize_training()\n",
    "    gmm.means = [0.4627451, 0.10196079, 0.027450981, 0.011764706, 0.1254902]\n",
    "    likelihood = gmm.likelihood()\n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gmm_likelihood_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def gmm_joint_prob_test():\n",
    "    \"\"\"Testing the GMM method\n",
    "    for calculating the joint \n",
    "    log probability of a given point.\n",
    "    Should return -0.98196.\n",
    "    \n",
    "    returns:\n",
    "    joint_prob = float\n",
    "    \"\"\"\n",
    "    image_file = 'images/party_spock.png'\n",
    "    image_matrix = image_to_matrix(image_file)\n",
    "    num_components = 5\n",
    "    gmm = GaussianMixtureModel(image_matrix, num_components)\n",
    "    gmm.initialize_training()\n",
    "    gmm.means = [0.4627451, 0.10196079, 0.027450981, 0.011764706, 0.1254902]\n",
    "    test_val = 0.4627451\n",
    "    joint_prob = gmm.joint_prob(0.4627451)\n",
    "    return joint_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gmm_joint_prob_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generate_test_mixture(data_size, means, variances, mixing_coefficients):\n",
    "    \"\"\"\n",
    "    Generate synthetic test\n",
    "    data for a GMM based on\n",
    "    fixed means, variances and\n",
    "    mixing coefficients.\n",
    "    \n",
    "    params:\n",
    "    data_size = (int)\n",
    "    means = [float]\n",
    "    variances = [float]\n",
    "    mixing_coefficients = [float]\n",
    "    \n",
    "    returns:\n",
    "    data = np.array[float]\n",
    "    \"\"\"\n",
    "\n",
    "    data = np.zeros(data_size).flatten()\n",
    "\n",
    "    indices = np.random.choice( len(means), len(data), p=mixing_coefficients)\n",
    "\n",
    "    for i in range(len(indices)):\n",
    "        data[i] = np.random.normal(means[indices[i]], variances[indices[i]])\n",
    "\n",
    "    return np.array([data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def gmm_train_test():\n",
    "    \"\"\"Test the training \n",
    "    procedure for GMM using\n",
    "    synthetic data.\n",
    "    \n",
    "    returns:\n",
    "    gmm = GaussianMixtureModel\n",
    "    \"\"\"\n",
    "\n",
    "    print( 'Synthetic example with 2 means')\n",
    "\n",
    "    num_components = 2\n",
    "    data_range = (1,1000)\n",
    "    actual_means = [2, 4]\n",
    "    actual_variances = [1]*num_components\n",
    "    actual_mixing = [.5]*num_components\n",
    "    dataset_1 = generate_test_mixture(data_range, actual_means, actual_variances, actual_mixing)\n",
    "    gmm = GaussianMixtureModel(dataset_1, num_components)\n",
    "    gmm.initialize_training()\n",
    "    # start off with faulty means\n",
    "    gmm.means = [1,3]\n",
    "    initial_likelihood = gmm.likelihood()\n",
    "\n",
    "    gmm.train_model()\n",
    "    final_likelihood = gmm.likelihood()\n",
    "    likelihood_difference = final_likelihood - initial_likelihood\n",
    "    likelihood_thresh = 250\n",
    "    if(likelihood_difference >= likelihood_thresh):\n",
    "        print('Congrats! Your model\\'s log likelihood improved by at least %d.'%(likelihood_thresh))\n",
    "\n",
    "    print( 'Synthetic example with 4 means:')\n",
    "\n",
    "    num_components = 4\n",
    "    actual_means = [2,4,6,8]\n",
    "    actual_variances = [1]*num_components\n",
    "    actual_mixing = [.25]*num_components\n",
    "    dataset_1 = generate_test_mixture(data_range, \n",
    "                actual_means, actual_variances, actual_mixing)\n",
    "    gmm = GaussianMixtureModel(dataset_1, num_components)\n",
    "    gmm.initialize_training()\n",
    "    # start off with faulty means\n",
    "    gmm.means = [1,3,5,9]\n",
    "    initial_likelihood = gmm.likelihood()\n",
    "    gmm.train_model()\n",
    "    final_likelihood = gmm.likelihood()\n",
    "    \n",
    "    # compare likelihoods\n",
    "    likelihood_difference = final_likelihood - initial_likelihood\n",
    "    likelihood_thresh = 200\n",
    "    if(likelihood_difference >= likelihood_thresh):\n",
    "        print('Congrats! Your model\\'s log likelihood improved by at least %d.'%(likelihood_thresh))\n",
    "    return gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gmm_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def gmm_segment_test():\n",
    "    \"\"\"\n",
    "    Apply the trained GMM \n",
    "    to unsegmented image and\n",
    "    generate a segmented image.\n",
    "    \n",
    "    returns:\n",
    "    segmented_matrix = numpy.ndarray[numpy.ndarray[float]]\n",
    "    \"\"\"\n",
    "    image_file = 'images/party_spock.png'\n",
    "    image_matrix = image_to_matrix(image_file)\n",
    "    num_components = 3\n",
    "    gmm = GaussianMixtureModel(image_matrix, num_components)\n",
    "    gmm.initialize_training()\n",
    "    gmm.train_model()\n",
    "    segment = gmm.segment()\n",
    "    segment_num_components = len(np.unique(segment))\n",
    "    if(segment_num_components == num_components):\n",
    "        print('Congrats! Your segmentation produced an image '+\n",
    "              'with the correct number of components.')\n",
    "    return segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def gmm_best_segment_test():\n",
    "    \"\"\"\n",
    "    Calculate the best segment\n",
    "    generated by the GMM and\n",
    "    compare the subsequent likelihood\n",
    "    of a reference segmentation. \n",
    "    Note: this test will take a while \n",
    "    to run.\n",
    "    \n",
    "    returns:\n",
    "    best_seg = np.ndarray[np.ndarray[float]]\n",
    "    \"\"\"\n",
    "    image_file = 'images/party_spock.png'\n",
    "    image_matrix = image_to_matrix(image_file)\n",
    "    image_matrix_flat = flatten_image_matrix(image_matrix)\n",
    "    num_components = 3\n",
    "    gmm = GaussianMixtureModel(image_matrix, num_components)\n",
    "    gmm.initialize_training()\n",
    "    iters = 10\n",
    "    # generate best segment from 10 iterations\n",
    "    # and extract its likelihood\n",
    "    best_seg = gmm.best_segment(iters)\n",
    "    matrix_to_image(best_seg, 'images/best_segment_spock.png')\n",
    "    best_likelihood = gmm.likelihood()\n",
    "    \n",
    "    # extract likelihood from reference image\n",
    "    ref_image_file = 'images/party_spock%d_baseline.png'%(num_components)\n",
    "    ref_image = image_to_matrix(ref_image_file, grays=True)\n",
    "    gmm_ref = GaussianMixtureModel(ref_image, num_components)\n",
    "    ref_vals = ref_image.flatten()\n",
    "    ref_means = list(set(ref_vals))\n",
    "    ref_variances = [0]*num_components\n",
    "    ref_mixing = [0]*num_components\n",
    "    for i in range(num_components):\n",
    "        relevant_vals = ref_vals[ref_vals==ref_means[i]]\n",
    "        ref_mixing[i] = float(len(relevant_vals)) / float(len(ref_vals))\n",
    "        ref_variances[i] = np.mean((image_matrix_flat[ref_vals==ref_means[i]] - ref_means[i])**2)\n",
    "    gmm_ref.means = ref_means\n",
    "    gmm_ref.variances = ref_variances\n",
    "    gmm_ref.mixing_coefficients = ref_mixing\n",
    "    ref_likelihood = gmm_ref.likelihood()\n",
    "    \n",
    "    # compare best likelihood and reference likelihood\n",
    "    likelihood_diff = best_likelihood - ref_likelihood\n",
    "    likelihood_thresh = 1e4\n",
    "    if(likelihood_diff >= likelihood_thresh):\n",
    "        print('Congrats! Your image segmentation is an improvement over ' +\n",
    "              'the baseline by at least %.2f.'%(likelihood_thresh))\n",
    "    return best_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "best_segment = gmm_best_segment_test()\n",
    "matrix_to_image(best_segment, 'best_segment.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Part 3: Model experimentation\n",
    "--\n",
    "20 points\n",
    "\n",
    "We'll now experiment with a few methods for improving GMM performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "3a: Improved initialization\n",
    "--\n",
    "12.5 points\n",
    "\n",
    "To run EM in our baseline Gaussian mixture model, we use random initialization to determine the initial values for our component means. We can do better than this!\n",
    "\n",
    "Fill in the below `GaussianMixtureModelImproved.initialize_training()` with an improvement in component initialization. Please don't use any external packages for anything other than basic calculations (e.g. `scipy.misc.logsumexp`). Note that your improvement might significantly slow down runtime, although we don't expect you to spend more than 10 minutes on initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Hint: you'll probably want an unsupervised learning method to initialize your component means. Clustering is one useful example of unsupervised learning, and you may want to look at 1-dimensional methods such as [Jenks natural breaks optimization](https://en.wikipedia.org/wiki/Jenks_natural_breaks_optimization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class GaussianMixtureModelImproved(GaussianMixtureModel):\n",
    "    \"\"\"A Gaussian mixture model\n",
    "    for a provided grayscale image, \n",
    "    with improved training \n",
    "    performance.\"\"\"\n",
    "    \n",
    "    def initialize_training(self):\n",
    "        \"\"\"\n",
    "        Initialize the training\n",
    "        process by setting each\n",
    "        component mean using some algorithm that you think might give better means to start with,\n",
    "        each component variance to 1, and\n",
    "        each component mixing coefficient\n",
    "        to a uniform value \n",
    "        (e.g. 4 components -> [0.25,0.25,0.25,0.25]).\n",
    "        [You can feel free to modify the variance and mixing coefficient initializations too if that works well.]\n",
    "        \"\"\"\n",
    "        # TODO: finish this\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def gmm_improvement_test():\n",
    "    \"\"\"\n",
    "    Tests whether the new mixture \n",
    "    model is actually an improvement\n",
    "    over the previous one: if the\n",
    "    new model has a higher likelihood\n",
    "    than the previous model for the\n",
    "    provided initial means.\n",
    "    \n",
    "    returns:\n",
    "    original_segment = numpy.ndarray[numpy.ndarray[float]]\n",
    "    improved_segment = numpy.ndarray[numpy.ndarray[float]]\n",
    "    \"\"\"\n",
    "    image_file = 'images/party_spock.png'\n",
    "    image_matrix = image_to_matrix(image_file)\n",
    "    num_components = 3\n",
    "    initial_means = [0.4627451, 0.20392157, 0.36078432]\n",
    "    # first train original model with fixed means\n",
    "    gmm = GaussianMixtureModel(image_matrix, num_components)\n",
    "    gmm.initialize_training()\n",
    "    gmm.means = np.copy(initial_means)\n",
    "    gmm.train_model()\n",
    "    original_segment = gmm.segment()\n",
    "    original_likelihood = gmm.likelihood()\n",
    "    # then train improved model\n",
    "    gmm_improved = GaussianMixtureModelImproved(image_matrix, num_components)\n",
    "    gmm_improved.initialize_training()\n",
    "    gmm_improved.train_model()\n",
    "    improved_segment = gmm_improved.segment()\n",
    "    improved_likelihood = gmm_improved.likelihood()\n",
    "    # then calculate likelihood difference\n",
    "    diff_thresh = 1e3\n",
    "    likelihood_diff = improved_likelihood - original_likelihood\n",
    "    if(likelihood_diff >= diff_thresh):\n",
    "        print('Congrats! Improved model scores a likelihood that was at ' +\n",
    "              'least %d higher than the original model.'%(diff_thresh))\n",
    "    return original_segment, improved_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "best_segment, best_segment_improved = gmm_improvement_test()\n",
    "matrix_to_image(best_segment, 'best_segment_original.png')\n",
    "matrix_to_image(best_segment_improved, 'best_segment_improved.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "3b: Convergence condition\n",
    "--\n",
    "7.5 points\n",
    "\n",
    "You might be skeptical of the convergence criterion we've provided in `default_convergence()`. To test out another convergence condition, implement `new_convergence_condition()` to return true if all the new model parameters (means, variances, and mixing coefficients) are within 10% of the previous variables for 10 consecutive iterations. This will mean re-implementing `train_model()`, which you will also do below in `GaussianMixtureModelConvergence`. \n",
    "\n",
    "You can compare the two convergence functions in `convergence_condition_test()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def new_convergence_function(previous_variables, new_variables, conv_ctr, conv_ctr_cap=10):\n",
    "    \"\"\"\n",
    "    Convergence function\n",
    "    based on parameters:\n",
    "    when all variables vary by\n",
    "    less than 10% from the previous\n",
    "    iteration's variables, increase\n",
    "    the convergence counter.\n",
    "    \n",
    "    params:\n",
    "    \n",
    "    previous_variables = [numpy.ndarray[float]] containing [means, variances, mixing_coefficients]\n",
    "    new_variables = [numpy.ndarray[float]] containing [means, variances, mixing_coefficients]\n",
    "    conv_ctr = int\n",
    "    conv_ctr_cap = int\n",
    "    \n",
    "    return:\n",
    "    conv_ctr = int\n",
    "    converged = boolean\n",
    "    \"\"\"\n",
    "    # TODO: finish this function\n",
    "    raise NotImplementedError()\n",
    "    return conv_ctr, converged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class GaussianMixtureModelConvergence(GaussianMixtureModel):\n",
    "    \"\"\"\n",
    "    Class to test the \n",
    "    new convergence function\n",
    "    in the same GMM model as\n",
    "    before.\n",
    "    \"\"\"\n",
    "\n",
    "    def train_model(self, convergence_function=new_convergence_function):\n",
    "        # TODO: finish this function\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def convergence_condition_test():\n",
    "    \"\"\"\n",
    "    Compare the performance of \n",
    "    the default convergence function\n",
    "    with the new convergence function.\n",
    "    \n",
    "    return:\n",
    "    default_convergence_likelihood = float\n",
    "    new_convergence_likelihood = float\n",
    "    \"\"\"\n",
    "    image_file = 'images/party_spock.png'\n",
    "    image_matrix = image_to_matrix(image_file)\n",
    "    num_components = 3\n",
    "    initial_means = [0.4627451, 0.10196079, 0.027450981]\n",
    "    \n",
    "    # first test original model\n",
    "    gmm = GaussianMixtureModel(image_matrix, num_components)\n",
    "    gmm.initialize_training()\n",
    "    gmm.means = np.copy(initial_means)\n",
    "    gmm.train_model()\n",
    "    default_convergence_likelihood = gmm.likelihood()\n",
    "    \n",
    "    # now test new convergence model\n",
    "    gmm_new = GaussianMixtureModelConvergence(image_matrix, num_components)\n",
    "    gmm_new.initialize_training()\n",
    "    gmm_new.means = np.copy(initial_means)\n",
    "    gmm_new.train_model()\n",
    "    new_convergence_likelihood = gmm_new.likelihood()\n",
    "    \n",
    "    # test convergence difference\n",
    "    convergence_diff = new_convergence_likelihood - default_convergence_likelihood\n",
    "    convergence_thresh = 200\n",
    "    if(convergence_diff >= convergence_thresh):\n",
    "        print('Congrats! The likelihood difference between the original '\n",
    "              + 'and the new convergence models should be at least %.2f'%(convergence_thresh))\n",
    "    return default_convergence_likelihood, new_convergence_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Part 4: Bayesian information criterion\n",
    "-- \n",
    "20 points\n",
    "\n",
    "In our previous solutions, our only criterion for choosing a model was whether it maximizes the posterior likelihood regardless of how many parameters this requires. As a result, the \"best\" model may simply be the model with the most parameters, which would be overfit to the training data.\n",
    "\n",
    "To avoid overfitting, we can use the [Bayesian information criterion](https://en.wikipedia.org/wiki/Bayesian_information_criterion) (a.k.a. BIC) which penalizes models based on the number of parameters they use. In the case of the Gaussian mixture model, this is equal to the number of components times the number of variables per component (mean, variance and mixing coefficient) = 3\\*components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "4a: Implement BIC\n",
    "--\n",
    "5 points\n",
    "\n",
    "Implement `bayes_info_criterion()` to calculate the BIC of a trained `GaussianMixtureModel`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def bayes_info_criterion(gmm):\n",
    "    # TODO: finish this function\n",
    "    raise NotImplementedError()\n",
    "    return BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def bayes_info_test():\n",
    "    \"\"\"\n",
    "    Test for your\n",
    "    implementation of\n",
    "    BIC on fixed GMM values.\n",
    "    Should be about 727045.\n",
    "    \n",
    "    returns:\n",
    "    BIC = float\n",
    "    \"\"\"\n",
    "    \n",
    "    image_file = 'images/party_spock.png'\n",
    "    image_matrix = image_to_matrix(image_file)\n",
    "    num_components = 3\n",
    "    initial_means = [0.4627451, 0.10196079, 0.027450981]\n",
    "    gmm = GaussianMixtureModel(image_matrix, num_components)\n",
    "    gmm.initialize_training()\n",
    "    gmm.means = np.copy(initial_means)\n",
    "    BIC = bayes_info_criterion(gmm)\n",
    "    return BIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "4b: Test BIC\n",
    "-- \n",
    "15 points\n",
    "\n",
    "Now implement `BIC_model_test()`, in which you will use the BIC and likelihood to determine the optimal number of components in the Party Spock image. Use the original `GaussianMixtureModel` for your models. Iterate from k=2 to k=7 and use the provided means to train a model that minimizes its BIC and a model that maximizes its likelihood.\n",
    "\n",
    "Then, fill out `BIC_likelihood_question()` to return the number of components in both the min-BIC and the max-likelihood model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def BIC_likelihood_model_test():\n",
    "    \"\"\"Test to compare the \n",
    "    models with the lowest BIC\n",
    "    and the highest likelihood.\n",
    "    \n",
    "    returns:\n",
    "    min_BIC_model = GaussianMixtureModel\n",
    "    max_likelihood_model = GaussianMixtureModel\n",
    "    \"\"\"\n",
    "    # TODO: finish this method\n",
    "    raise NotImplementedError()\n",
    "    comp_means = [\n",
    "        [0.023529412, 0.1254902],\n",
    "        [0.023529412, 0.1254902, 0.20392157],\n",
    "        [0.023529412, 0.1254902, 0.20392157, 0.36078432],\n",
    "        [0.023529412, 0.1254902, 0.20392157, 0.36078432, 0.59215689],\n",
    "        [0.023529412, 0.1254902, 0.20392157, 0.36078432, 0.59215689, 0.71372563],\n",
    "        [0.023529412, 0.1254902, 0.20392157, 0.36078432, 0.59215689, 0.71372563, 0.964706]\n",
    "    ]\n",
    "    return min_BIC_model, max_likelihood_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def BIC_likelihood_question():\n",
    "    \"\"\"\n",
    "    Choose the best number of\n",
    "    components for each metric\n",
    "    (min BIC and maximum likelihood).\n",
    "    \n",
    "    returns:\n",
    "    pairs = dict\n",
    "    \"\"\"\n",
    "    # TODO: fill in bic and likelihood\n",
    "    raise NotImplementedError()\n",
    "    bic = 0\n",
    "    likelihood = 0\n",
    "    pairs = {\n",
    "        'BIC' : bic,\n",
    "        'likelihood' : likelihood \n",
    "    }\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Bonus\n",
    "-- \n",
    "2 points\n",
    "\n",
    "A crucial part of machine learning is working with very large datasets. As stated before, using for loops over these datasets will result in the code taking many hours, or even several days, to run. Even vectorization can take time if not done properly, and as such there are certain tricks you can perform to get your code to run as fast as physically possible.\n",
    "\n",
    "For this part of the assignment, you will need to implement part of a k-Means algorithm. You are given two arrays  - `points_array` with *X* n-dimensional points, and `means_array` with *Y* n-dimensional points.\n",
    "You will need to return an *X* x *Y* array containing the distances from each point in `points_array` to each point in `means_array`.\n",
    "\n",
    "#### Your code will be tested using two **very** large arrays, against our reference implementation, which was designed by Murtaza Dhuliawala. Thus, you'll be competing against the Head TA! \n",
    "\n",
    "If your implementation returns the correct answer in time comparable to Murtaza's implementation, you will receive 2 bonus points.\n",
    "\n",
    "For reference, the data used is in the order of thousands of points and hundreds of means, and Bonnie automatically kills a grading script that takes more than 500MB. So please test accordingly locally before submitting, as you may lose a submission for an inefficient solution. It is very likely that you could run out of memory if your implementation is inefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def bonus(points_array, means_array):\n",
    "    \"\"\"\n",
    "    Return the distance from every point in points_array\n",
    "    to every point in means_array.\n",
    "    \n",
    "    returns:\n",
    "    dists = numpy array of float\n",
    "    \"\"\"\n",
    "    # TODO: fill in the bonus function\n",
    "    # REMOVE THE LINE BELOW IF ATTEMPTING BONUS\n",
    "    raise NotImplementedError()\n",
    "    return dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def bonus_test():\n",
    "    points = np.array([[ 0.9059608,0.67550357,0.13525533],[ 0.23656114,0.63624466,0.3606615 ],[ 0.91163215,0.24431103,0.33318504],[ 0.25209736,0.24600123,0.42392935],[ 0.62799146,0.04520208,0.55232494],[ 0.5588561, 0.06397713,0.53465371],[ 0.82530045,0.62811624,0.79672349],[ 0.50048147,0.13215356,0.54517893],[ 0.84725662,0.71085917,0.61111105],[ 0.25236734,0.25951904,0.70239158]])\n",
    "    means = np.array([[ 0.39874413,0.47440682,0.86140829],[ 0.05671347,0.26599323,0.33577454],[ 0.7969679, 0.44920099,0.37978416],[ 0.45428452,0.51414022,0.21209852],[ 0.7112214, 0.94906158,0.25496493]])\n",
    "    expected_answer = np.array([[ 0.90829883,0.9639127, 0.35055193,0.48575144,0.35649377],[ 0.55067427,0.41237201,0.59110637,0.29048911,0.57821151],[ 0.77137409,0.8551975, 0.23937264,0.54464354,0.73685561],[ 0.51484192,0.21528078,0.58320052,0.39705222,0.85652654],[ 0.57645778,0.64961631,0.47067874,0.60483973,0.95515036],[ 0.54850426,0.57663736,0.47862222,0.56358129,0.94064631],[ 0.45799673,0.966609,0.45458971,0.70173336,0.63993928],[ 0.47695785,0.50861901,0.46451987,0.50891112,0.89217387],[ 0.56543953,0.94798437,0.35285421,0.59357932,0.4495398 ],[ 0.30477736,0.41560848,0.66079087,0.58820896,0.94138546]])\n",
    "    if np.allclose(expected_answer,bonus(points,means),1e-7):\n",
    "        print 'You returned the correct distances.'\n",
    "    else:\n",
    "        print 'Your distance calculation is incorrect.'\n",
    "\n",
    "bonus_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You're done with the requirements! Hope you have completed the functions in the mixture_models.py file and tested everything!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
